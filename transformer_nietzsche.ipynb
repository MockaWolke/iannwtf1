{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2z8lTtUxfyhk"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow_text\n",
        "!pip install -q sentencepiece\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_txt\n",
        "import tqdm.notebook as note\n",
        "import sentencepiece as sp\n",
        "import io\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPERPARAMS\n",
        "VOCAB_SIZE = 2000\n",
        "WINDOW_SIZE = 33\n",
        "SHUFFLE_SIZE = 1000\n",
        "BATCH_SIZE = 50\n",
        "D_EMBEDDINGS = 64\n",
        "NUM_HEADS = 2\n",
        "KEY_DIM = 3 #???"
      ],
      "metadata": {
        "id": "N2IkveMblaFu"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save filepath to downloaded \"Beyond Good and Evil\"\n",
        "path = tf.keras.utils.get_file(\"nietzsche.txt\", origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
        "# Load txt into str\n",
        "text = open(path).read()\n",
        "\n",
        "# Train the tokenizer\n",
        "sp.SentencePieceTrainer.train(\n",
        "    input=path, model_prefix='tokenizer_model', model_type=\"unigram\", vocab_size=VOCAB_SIZE)\n",
        "# deserialize the trained model file to load it in the correct format\n",
        "trained_tokenizer_model = tf.io.gfile.GFile('tokenizer_model.model', \"rb\").read()\n",
        "# load the model as a tokenizer that can be used inside a tensorflow model\n",
        "tokenizer = tf_txt.SentencepieceTokenizer(\n",
        "    model=trained_tokenizer_model, out_type=tf.int32, nbest_size=-1, alpha=1, reverse=False,\n",
        "    add_bos=False, add_eos=False, return_nbest=False, name=None\n",
        ")\n",
        "\n",
        "# Tokenize the str\n",
        "tokens = tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "13_vzs9VhSLj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_windows = tf_txt.sliding_window(data=tokens, width=WINDOW_SIZE)\n",
        "token_ds = tf.data.Dataset.from_tensor_slices({\"input\": token_windows[:,:-1], \"target\": token_windows[:,-1]})\n",
        "token_ds = token_ds.shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "FI4SKvXzoSiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding_Layer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(Embedding_Layer, self).__init__()\n",
        "\n",
        "        self.embedding_1 = tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=D_EMBEDDINGS)\n",
        "        self.embedding_2 = tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=D_EMBEDDINGS)\n",
        "    \n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        zeros = self.embedding_1(tf.range(start=0, limit=WINDOW_SIZE-1))\n",
        "\n",
        "        return zeros + self.embedding_2(x)"
      ],
      "metadata": {
        "id": "u5DroX22wNKQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=KEY_DIM)\n",
        "        self.dense_1 = tf.keras.layers.Dense(units=32, activation=\"relu\")\n",
        "        self.dense_2 = tf.keras.layers.Dense(units=D_EMBEDDINGS)\n",
        "        self.dropout_1 = tf.keras.layers.Dropout(rate=0.1)\n",
        "        self.dropout_2 = tf.keras.layers.Dropout(rate=0.1)\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        att_out = self.mha(x)\n",
        "        att_out = self.dropout_2(att_out)\n",
        "        ln_out = self.norm_1(x + att_out)\n",
        "        ffn_out = self.dense_1(ln_out)\n",
        "        ffn_out = self.dense_2(ffn_out)\n",
        "        ffn_out = self.dropout_2(ffn_out)\n",
        "\n",
        "        return self.norm_2(ln_out + ffn_out)"
      ],
      "metadata": {
        "id": "LCT7cjNX0xjx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, tokenizer):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.optimizer = tf.keras.optimizers.Adam\n",
        "        self.loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        self.pos_embedding = Embedding_Layer\n",
        "        self.block = TransformerBlock\n",
        "        self.pool = tf.keras.layers.GlobalAvgPool1D\n",
        "        self.dense = tf.keras.layers.Dense(units=VOCAB_SIZE)\n",
        "\n",
        "        self.metrics = [\n",
        "                        tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                        tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "                        tf.keras.metrics.TopKCategoricalAccuracy(3,name=\"top-3-acc\") \n",
        "                        ]\n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "        \n",
        "        return self.pos_embedding(self.block(self.pool(self.dense(x))))\n",
        "\n",
        "\n",
        "    def reset_metrics(self):\n",
        "        for metric in self.metrics:\n",
        "            metric.reset_states()\n",
        "            \n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        \n",
        "        x, targets = data[\"input\"], data[\"target\"]\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(x, training=True)\n",
        "            loss = self.loss_func(targets, predictions) + tf.reduce_sum(self.losses)\n",
        "        \n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        # update loss metric\n",
        "        self.metrics[0].update_state(loss)\n",
        "        \n",
        "        # for all metrics except loss, update states (accuracy etc.)\n",
        "        for metric in self.metrics[1:]:\n",
        "            metric.update_state(targets,predictions)\n",
        "\n",
        "        # Return a dictionary mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def generate_text(self, prompt):\n",
        "        tokens = self.tokenizer.tokenize(prompt)\n",
        "        tokens = tf.pad(tokens, [-1 * ((-1 * (32 - tokens.shape[0])) // 2), (32 - tokens.shape[0]) // 2], \"CONSTANT\")\n",
        "        tokens = tf.expand_dims(tokens, axis=-1)\n",
        "        logits = tf.math.top_k(tokens, sorted=True)\n",
        "        return tf.random.categorical(logits=logits)"
      ],
      "metadata": {
        "id": "cmuDMe285XAs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}