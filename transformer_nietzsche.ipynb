{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_nietzsche.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "526335d1e1c6454aae698febf86d327c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6f68edbc2d544d1a9e545849e663c0ed",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0fbd55279d4b488a87960255ae88501c",
              "IPY_MODEL_3590076cdae64c52b4ef9ac66b01bc0d",
              "IPY_MODEL_ca78dd3dcb954b45af4f9fc3a7ac62ba"
            ]
          }
        },
        "6f68edbc2d544d1a9e545849e663c0ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0fbd55279d4b488a87960255ae88501c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_25ba6f3312934c17a1cc6b942cf581fe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e72902ed68b84204aee0875612ec4316"
          }
        },
        "3590076cdae64c52b4ef9ac66b01bc0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b2496cdddea64a96ba8f0cee9be1be9d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 3429,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ee544523abf94bb9a2e79124779ed65b"
          }
        },
        "ca78dd3dcb954b45af4f9fc3a7ac62ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f1baec8cfb5e4c538235b2105e7d1328",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/3429 [00:06&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_91d30f78ad92493bb538674dedfb2df5"
          }
        },
        "25ba6f3312934c17a1cc6b942cf581fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e72902ed68b84204aee0875612ec4316": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2496cdddea64a96ba8f0cee9be1be9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ee544523abf94bb9a2e79124779ed65b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1baec8cfb5e4c538235b2105e7d1328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "91d30f78ad92493bb538674dedfb2df5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Use this if the packages are not installed yet\n",
        "!pip install -q tensorflow_text\n",
        "!pip install -q sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcrOP5CEPxtG",
        "outputId": "015b09c0-ee24-4077-c3fa-07ede5f8db2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.9 MB 12.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 497.5 MB 28 kB/s \n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 30.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 37.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 49.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 46.0 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 12.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2z8lTtUxfyhk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_txt\n",
        "import tqdm.notebook as note\n",
        "import sentencepiece as sp\n",
        "import io\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPERPARAMS\n",
        "VOCAB_SIZE = 2000\n",
        "SEQ_LEN = 32\n",
        "SHUFFLE_SIZE = 1000\n",
        "BATCH_SIZE = 50\n",
        "D_EMBEDDINGS = 64\n",
        "NUM_HEADS = 3\n",
        "TOP_K = 5"
      ],
      "metadata": {
        "id": "N2IkveMblaFu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save filepath to downloaded \"Beyond Good and Evil\"\n",
        "path = tf.keras.utils.get_file(\"nietzsche.txt\", origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
        "# Load txt into str\n",
        "text = open(path).read()\n",
        "\n",
        "# Train the tokenizer\n",
        "sp.SentencePieceTrainer.train(\n",
        "    input=path, model_prefix='tokenizer_model', model_type=\"unigram\", vocab_size=VOCAB_SIZE)\n",
        "# deserialize the trained model file to load it in the correct format\n",
        "trained_tokenizer_model = tf.io.gfile.GFile('tokenizer_model.model', \"rb\").read()\n",
        "# load the model as a tokenizer that can be used inside a tensorflow model\n",
        "tokenizer = tf_txt.SentencepieceTokenizer(\n",
        "    model=trained_tokenizer_model, out_type=tf.int32, nbest_size=-1, alpha=1, reverse=False,\n",
        "    add_bos=False, add_eos=False, return_nbest=False, name=None\n",
        ")\n",
        "\n",
        "# Tokenize the str\n",
        "tokens = tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "13_vzs9VhSLj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_windows = tf_txt.sliding_window(data=tokens, width=SEQ_LEN+1)\n",
        "token_ds = tf.data.Dataset.from_tensor_slices({\"input\": token_windows[:,:-1], \"target\": token_windows[:,-1]})\n",
        "token_ds = token_ds.shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "FI4SKvXzoSiU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding_Layer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(Embedding_Layer, self).__init__()\n",
        "\n",
        "        self.embedding_1 = tf.keras.layers.Embedding(input_dim=SEQ_LEN, output_dim=D_EMBEDDINGS)\n",
        "        self.embedding_2 = tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=D_EMBEDDINGS)\n",
        "    \n",
        "    \n",
        "    def call(self, x):\n",
        "        zeros = self.embedding_1(tf.range(start=0, limit=SEQ_LEN))\n",
        "\n",
        "        return zeros + self.embedding_2(x)"
      ],
      "metadata": {
        "id": "u5DroX22wNKQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=D_EMBEDDINGS)\n",
        "        self.dense_1 = tf.keras.layers.Dense(units=32, activation=\"relu\")\n",
        "        self.dense_2 = tf.keras.layers.Dense(units=D_EMBEDDINGS)\n",
        "        self.dropout_1 = tf.keras.layers.Dropout(rate=0.1)\n",
        "        self.dropout_2 = tf.keras.layers.Dropout(rate=0.1)\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        att_out = self.mha(x, x)\n",
        "        att_out = self.dropout_2(att_out)\n",
        "        ln_out = self.norm_1(x + att_out)\n",
        "        ffn_out = self.dense_1(ln_out)\n",
        "        ffn_out = self.dense_2(ffn_out)\n",
        "        ffn_out = self.dropout_2(ffn_out)\n",
        "\n",
        "        return self.norm_2(ln_out + ffn_out)"
      ],
      "metadata": {
        "id": "LCT7cjNX0xjx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, tokenizer):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.optimizer = tf.keras.optimizers.Adam()\n",
        "        self.loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        self.pos_embedding = Embedding_Layer()\n",
        "        self.block = TransformerBlock()\n",
        "        self.pool = tf.keras.layers.GlobalAvgPool1D()\n",
        "        self.dense = tf.keras.layers.Dense(units=VOCAB_SIZE)\n",
        "\n",
        "        self.metrics_list = [\n",
        "                        tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                        tf.keras.metrics.CategoricalAccuracy(name=\"acc\")\n",
        "                        ]\n",
        "\n",
        "    def call(self, x):\n",
        "        embedded = self.pos_embedding(x)\n",
        "        embedded = self.block(embedded)\n",
        "        embedded = self.pool(embedded)\n",
        "\n",
        "        return self.dense(embedded)\n",
        "\n",
        "\n",
        "    def reset_metrics(self):\n",
        "        for metric in self.metrics_list:\n",
        "            metric.reset_states()\n",
        "            \n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        \n",
        "        x, targets = data[\"input\"], data[\"target\"]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(x)\n",
        "            loss = self.loss_func(targets, predictions) + tf.reduce_sum(self.losses)\n",
        "        \n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        #for g, v in zip(gradients, self.trainable_variables):\n",
        "            #print(\"gradient: \", g.shape, \" var: \", v.shape, \"\\n\")\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        # update metrics\n",
        "        self.metrics_list[0].update_state(loss) # loss\n",
        "        self.metrics_list[1].update_state(targets, predictions) # acc\n",
        "\n",
        "        # Return a dictionary mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics_list}\n",
        "\n",
        "    def generate_text(self, prompt, sample_size=5):\n",
        "        tokens = self.tokenizer.tokenize(prompt)\n",
        "        prompt_len = tokens.shape[0]\n",
        "        for _ in sample_size:\n",
        "            tokens = tf.pad(tokens, [SEQ_LEN - prompt_len,0], \"CONSTANT\", constant_values=-1)\n",
        "            tokens = tf.expand_dims(tokens, axis=-1)\n",
        "            logits, indices = tf.math.top_k(self(tokens), k=TOP_K, sorted=True)\n",
        "            sample = tf.random.categorical(logits, 1)\n",
        "            tokens = tf.concat(tokens[0], sample, -1)[-(prompt_len+sample.shape[0]):]\n",
        "        return f\"Generated the last {sample_size} words of the following text:\\n{self.tokonizer.detokenize(tokens)}\\n\""
      ],
      "metadata": {
        "id": "cmuDMe285XAs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "6gT1Kep3NCgk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(tokenizer)\n",
        "\n",
        "# Define where to save the log\n",
        "hyperparameter_string = \"VOCAB_SIZE-2000__SEQ_LEN-32__SHUFFLE_SIZE-1000__BATCH_SIZE-50__D_EMBEDDINGS-64__NUM_HEADS-3__TOP_K-5\"\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "log_path = f\"logs/{hyperparameter_string}/{current_time}/train\"\n",
        "summary_writer = tf.summary.create_file_writer(log_path)"
      ],
      "metadata": {
        "id": "ASd0FllmExMl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "sample_size = 5\n",
        "\n",
        "for epoch in range(5):\n",
        "    \n",
        "    print(f\"Epoch {epoch}:\")\n",
        "    \n",
        "    # Training:\n",
        "    \n",
        "    for data in note.tqdm(token_ds, position=0, leave=True):\n",
        "        metrics = model.train_step(data)\n",
        "        generated_text = model.generate_text(\"prompt\", sample_size)\n",
        "    \n",
        "    # print the metrics\n",
        "    print([f\"{key}: {value}\" for (key, value) in zip(list(metrics.keys()), list(metrics.values()))])\n",
        "    # print generated text\n",
        "    print(f\"Generated the last {sample_size} words of the following text:\\n{generated_text}\\n\")\n",
        "\n",
        "    with summary_writer.as_default():\n",
        "        # logging the metrics to the log file which is used by tensorboard\n",
        "        for metric in model.metrics_list:\n",
        "            tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "        # logging generated text\n",
        "        tf.summary.text(f\"sample_size-{sample_size}\", generated_text, step=epoch)\n",
        "    \n",
        "    # reset all metrics (requires a reset_metrics method in the model)\n",
        "    model.reset_metrics()\n",
        "    \n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484,
          "referenced_widgets": [
            "526335d1e1c6454aae698febf86d327c",
            "6f68edbc2d544d1a9e545849e663c0ed",
            "0fbd55279d4b488a87960255ae88501c",
            "3590076cdae64c52b4ef9ac66b01bc0d",
            "ca78dd3dcb954b45af4f9fc3a7ac62ba",
            "25ba6f3312934c17a1cc6b942cf581fe",
            "e72902ed68b84204aee0875612ec4316",
            "b2496cdddea64a96ba8f0cee9be1be9d",
            "ee544523abf94bb9a2e79124779ed65b",
            "f1baec8cfb5e4c538235b2105e7d1328",
            "91d30f78ad92493bb538674dedfb2df5"
          ]
        },
        "id": "1CE8nx1YEIHU",
        "outputId": "8d81e1ca-18d1-41ad-c626-bb76163e462d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "526335d1e1c6454aae698febf86d327c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3429 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-da1c82790724>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# print the metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2614cd00c918>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(self, prompt, sample_size)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mprompt_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_text/python/ops/sentencepiece_tokenizer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, input, name)\u001b[0m\n\u001b[1;32m    143\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mragged_conversion_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m           \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_text/python/ops/sentencepiece_tokenizer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, input, name)\u001b[0m\n\u001b[1;32m    152\u001b[0m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_bos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_eos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                   self.reverse, self.out_type, return_nbest=self.return_nbest))\n\u001b[0m\u001b[1;32m    155\u001b[0m           tokens = RaggedTensor.from_nested_row_splits(\n\u001b[1;32m    156\u001b[0m               \u001b[0mflat_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36msentencepiece_tokenize_op\u001b[0;34m(sp_handle, input, nbest_size, alpha, add_bos, add_eos, reverse, out_type, Tsplits, return_nbest, name)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7184\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7185\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7186\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/"
      ],
      "metadata": {
        "id": "SF_urQ9IPA6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model with a meaningful name\n",
        "model.save_weights(f\"saved_model_{hyperparameter_string}\", save_format=\"tf\")"
      ],
      "metadata": {
        "id": "5uWYBEFaICD4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}